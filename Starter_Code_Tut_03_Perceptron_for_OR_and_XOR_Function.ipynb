{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Perceptron and Perceptron Learning Algorithm."
      ],
      "metadata": {
        "id": "mMxE_MEWbFm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving \"OR\" Function with Perceptron."
      ],
      "metadata": {
        "id": "FXOPfEawbNrW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2aHe65pibCcZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    A simple Perceptron classifier for binary classification.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    weights : np.ndarray\n",
        "        Array of weights including the bias term, initialized randomly.\n",
        "    learning_rate : float\n",
        "        The step size for weight updates.\n",
        "    epochs : int\n",
        "        The number of training iterations over the dataset.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    step_function(z)\n",
        "        Activation function that returns 1 if z >= 0, else 0.\n",
        "\n",
        "    predict(x)\n",
        "        Predicts the output for a given input sample x.\n",
        "\n",
        "    train(X, Y)\n",
        "        Trains the perceptron on the given dataset (X: inputs, Y: target labels).\n",
        "\n",
        "    Example:\n",
        "    --------\n",
        "    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    >>> Y = np.array([0, 1, 1, 1])  # OR function\n",
        "    >>> perceptron = Perceptron(input_size=2, learning_rate=0.1, epochs=10)\n",
        "    >>> perceptron.train(X, Y)\n",
        "    >>> print(perceptron.predict([1, 1]))  # Expected output: 1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, learning_rate=0.1, epochs=10):\n",
        "        \"\"\"\n",
        "        Initializes the perceptron with small random weights.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        input_size : int\n",
        "            Number of input features.\n",
        "        learning_rate : float, optional (default=0.1)\n",
        "            Step size for weight updates.\n",
        "        epochs : int, optional (default=10)\n",
        "            Number of times to iterate over the dataset.\n",
        "        \"\"\"\n",
        "        self.weights = np.random.rand(input_size + 1) * 0.2 - 0.1  # Small random weights\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def step_function(self, z):\n",
        "        \"\"\"\n",
        "        Step activation function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        z : float\n",
        "            Weighted sum of inputs and weights.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        int\n",
        "            1 if z >= 0, else 0.\n",
        "        \"\"\"\n",
        "        return 1 if z >= 0 else 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Predicts the output for a given input sample.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x : array-like\n",
        "            Input feature vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        int\n",
        "            Predicted class label (0 or 1).\n",
        "        \"\"\"\n",
        "        x = np.insert(x, 0, 1)  # Bias term\n",
        "        z = np.dot(self.weights, x)\n",
        "        return self.step_function(z)\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        \"\"\"\n",
        "        Trains the perceptron using the perceptron learning rule.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            Training data (each row is an input sample).\n",
        "        Y : np.ndarray\n",
        "            Corresponding target labels (0 or 1).\n",
        "\n",
        "        Prints:\n",
        "        -------\n",
        "        Updates the weights and prints them after each epoch.\n",
        "        \"\"\"\n",
        "        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term to input\n",
        "        for epoch in range(self.epochs):\n",
        "            for i in range(X.shape[0]):\n",
        "                z = np.dot(self.weights, X[i])\n",
        "                y_pred = self.step_function(z)\n",
        "                error = Y[i] - y_pred\n",
        "                self.weights += self.learning_rate * error * X[i]\n",
        "            print(f\"Epoch {epoch+1}, Weights: {self.weights}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with adjustable input size\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "input_size = 3\n",
        "\n",
        "# Create training data (modify as needed for different functions and input sizes)\n",
        "X = np.array([\n",
        "    [0, 0, 0],\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 0],\n",
        "    [0, 1, 1],\n",
        "    [1, 0, 0],\n",
        "    [1, 0, 1],\n",
        "    [1, 1, 0],\n",
        "    [1, 1, 1],\n",
        "])\n",
        "\n",
        "# Example: OR function with 3 inputs\n",
        "Y = np.array([0, 1, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "# Initialize and train the perceptron\n",
        "perceptron = Perceptron(input_size, learning_rate=learning_rate, epochs=epochs)\n",
        "perceptron.train(X, Y)\n",
        "\n",
        "\n",
        "# Test the perceptron\n",
        "test_samples = X  # Use the same training samples for testing\n",
        "print(\"\\nPredictions:\")\n",
        "for sample in test_samples:\n",
        "    print(f\"Input: {sample}, Prediction: {perceptron.predict(sample)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSJESxmTbVK9",
        "outputId": "01fe1191-22ef-4a32-82ea-2d7c6e0aaf7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Weights: [0.19407706 0.08099116 0.00967708 0.05420585]\n",
            "Epoch 2, Weights: [0.09407706 0.08099116 0.00967708 0.05420585]\n",
            "Epoch 3, Weights: [-0.00592294  0.08099116  0.00967708  0.05420585]\n",
            "Epoch 4, Weights: [-0.00592294  0.08099116  0.00967708  0.05420585]\n",
            "Epoch 5, Weights: [-0.00592294  0.08099116  0.00967708  0.05420585]\n",
            "Epoch 6, Weights: [-0.00592294  0.08099116  0.00967708  0.05420585]\n",
            "Epoch 7, Weights: [-0.00592294  0.08099116  0.00967708  0.05420585]\n",
            "Epoch 8, Weights: [-0.00592294  0.08099116  0.00967708  0.05420585]\n",
            "Epoch 9, Weights: [-0.00592294  0.08099116  0.00967708  0.05420585]\n",
            "Epoch 10, Weights: [-0.00592294  0.08099116  0.00967708  0.05420585]\n",
            "\n",
            "Predictions:\n",
            "Input: [0 0 0], Prediction: 0\n",
            "Input: [0 0 1], Prediction: 1\n",
            "Input: [0 1 0], Prediction: 1\n",
            "Input: [0 1 1], Prediction: 1\n",
            "Input: [1 0 0], Prediction: 1\n",
            "Input: [1 0 1], Prediction: 1\n",
            "Input: [1 1 0], Prediction: 1\n",
            "Input: [1 1 1], Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying for \"XOR\" Function."
      ],
      "metadata": {
        "id": "O9R3B9Vrd6Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    A simple implementation of a Perceptron for binary classification.\n",
        "\n",
        "    This perceptron learns a linear decision boundary using the Perceptron Learning Rule.\n",
        "    It can be used to classify linearly separable datasets such as AND, OR, but not XOR.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    weights : np.ndarray\n",
        "        Weight vector including the bias term, initialized randomly.\n",
        "    learning_rate : float\n",
        "        The step size for weight updates during training.\n",
        "    epochs : int\n",
        "        Number of times the entire dataset is passed through during training.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    step_function(z)\n",
        "        Applies a step activation function (returns 1 if z >= 0, else 0).\n",
        "\n",
        "    predict(x)\n",
        "        Predicts the class label for a given input sample.\n",
        "\n",
        "    train(X, Y)\n",
        "        Trains the perceptron on a given dataset by updating weights iteratively.\n",
        "\n",
        "    Example:\n",
        "    --------\n",
        "    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    >>> Y = np.array([0, 1, 1, 1])  # OR function\n",
        "    >>> perceptron = Perceptron(input_size=2, learning_rate=0.1, epochs=10)\n",
        "    >>> perceptron.train(X, Y)\n",
        "    >>> print(perceptron.predict([1, 1]))  # Expected output: 1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, learning_rate=0.1, epochs=10):\n",
        "        \"\"\"\n",
        "        Initializes the perceptron model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        input_size : int\n",
        "            Number of input features (excluding the bias term).\n",
        "        learning_rate : float, optional (default=0.1)\n",
        "            Step size for weight updates.\n",
        "        epochs : int, optional (default=10)\n",
        "            Number of training iterations over the dataset.\n",
        "        \"\"\"\n",
        "        self.weights = np.random.rand(input_size + 1) * 0.2 - 0.1  # Small random weights\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def step_function(self, z):\n",
        "        \"\"\"\n",
        "        Step activation function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        z : float\n",
        "            Weighted sum of inputs and weights.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        int\n",
        "            1 if z >= 0, else 0.\n",
        "        \"\"\"\n",
        "        return 1 if z >= 0 else 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Predicts the output for a given input sample.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x : array-like\n",
        "            Input feature vector (excluding bias).\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        int\n",
        "            Predicted class label (0 or 1).\n",
        "        \"\"\"\n",
        "        x = np.insert(x, 0, 1)  # Insert bias term at the beginning\n",
        "        z = np.dot(self.weights, x)\n",
        "        return self.step_function(z)\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        \"\"\"\n",
        "        Trains the perceptron using the perceptron learning algorithm.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            Training data, where each row represents an input sample.\n",
        "        Y : np.ndarray\n",
        "            Target labels corresponding to the input samples (0 or 1).\n",
        "\n",
        "        Prints:\n",
        "        -------\n",
        "        Updates the weights and prints them after each epoch.\n",
        "        \"\"\"\n",
        "        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term to input\n",
        "        for epoch in range(self.epochs):\n",
        "            for i in range(X.shape[0]):\n",
        "                z = np.dot(self.weights, X[i])\n",
        "                y_pred = self.step_function(z)\n",
        "                error = Y[i] - y_pred\n",
        "                self.weights += self.learning_rate * error * X[i]\n",
        "            print(f\"Epoch {epoch+1}, Weights: {self.weights}\")\n"
      ],
      "metadata": {
        "id": "aOo9O63jbZFF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR data\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "Y = np.array([0, 1, 1, 0])  # XOR target\n",
        "\n",
        "# Initialize and train the perceptron\n",
        "input_size = X.shape[1]\n",
        "perceptron = Perceptron(input_size, epochs=100)  # Increased epochs for demonstration\n",
        "perceptron.train(X, Y)\n",
        "\n",
        "# Test the perceptron\n",
        "print(\"\\nXOR Predictions:\")\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]}, Prediction: {perceptron.predict(X[i])}, Target: {Y[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZVdgW3OeD7f",
        "outputId": "95f5dc4b-d0d5-477a-8822-b1b79300c46a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Weights: [-0.06963279 -0.04762103  0.03330859]\n",
            "Epoch 2, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 3, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 4, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 5, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 6, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 7, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 8, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 9, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 10, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 11, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 12, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 13, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 14, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 15, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 16, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 17, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 18, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 19, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 20, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 21, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 22, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 23, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 24, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 25, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 26, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 27, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 28, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 29, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 30, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 31, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 32, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 33, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 34, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 35, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 36, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 37, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 38, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 39, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 40, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 41, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 42, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 43, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 44, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 45, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 46, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 47, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 48, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 49, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 50, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 51, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 52, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 53, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 54, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 55, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 56, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 57, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 58, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 59, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 60, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 61, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 62, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 63, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 64, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 65, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 66, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 67, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 68, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 69, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 70, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 71, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 72, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 73, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 74, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 75, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 76, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 77, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 78, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 79, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 80, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 81, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 82, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 83, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 84, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 85, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 86, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 87, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 88, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 89, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 90, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 91, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 92, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 93, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 94, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 95, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 96, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 97, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 98, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 99, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "Epoch 100, Weights: [ 0.03036721 -0.04762103  0.03330859]\n",
            "\n",
            "XOR Predictions:\n",
            "Input: [0 0], Prediction: 1, Target: 0\n",
            "Input: [0 1], Prediction: 1, Target: 1\n",
            "Input: [1 0], Prediction: 0, Target: 1\n",
            "Input: [1 1], Prediction: 1, Target: 0\n"
          ]
        }
      ]
    }
  ]
}
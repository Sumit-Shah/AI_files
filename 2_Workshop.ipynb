{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  **Some Helper Function:**"
      ],
      "metadata": {
        "id": "kpi5f-NuuRbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Function:"
      ],
      "metadata": {
        "id": "NDqrxMpLuhLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoRF4EpdtF4D",
        "outputId": "24c68f52-098d-4303-c92f-88546ed46655"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dsjjjtw3vOgs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Artificial_Intelligence_and_Machine_Learning/Week_2_Components_of_Learning_From_Classification_Perspective/mnist_dataset.csv\") # changed to read_csv and the correct file name\n",
        "# Step 2: Dataset Information\n",
        "print(\"Dataset Preview:\")\n",
        "print(df.head())  # Show first 5 rows\n",
        "print(\"\\nDataset Information:\")\n",
        "print(df.info())  # Summary of dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-3U5SC0s2Wg",
        "outputId": "ccbce953-494f-47a1-9792-24a6b56d60ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Preview:\n",
            "   label  pixel_0  pixel_1  pixel_2  pixel_3  pixel_4  pixel_5  pixel_6  \\\n",
            "0      5        0        0        0        0        0        0        0   \n",
            "1      0        0        0        0        0        0        0        0   \n",
            "2      4        0        0        0        0        0        0        0   \n",
            "3      1        0        0        0        0        0        0        0   \n",
            "4      9        0        0        0        0        0        0        0   \n",
            "\n",
            "   pixel_7  pixel_8  ...  pixel_774  pixel_775  pixel_776  pixel_777  \\\n",
            "0        0        0  ...          0          0          0          0   \n",
            "1        0        0  ...          0          0          0          0   \n",
            "2        0        0  ...          0          0          0          0   \n",
            "3        0        0  ...          0          0          0          0   \n",
            "4        0        0  ...          0          0          0          0   \n",
            "\n",
            "   pixel_778  pixel_779  pixel_780  pixel_781  pixel_782  pixel_783  \n",
            "0          0          0          0          0          0          0  \n",
            "1          0          0          0          0          0          0  \n",
            "2          0          0          0          0          0          0  \n",
            "3          0          0          0          0          0          0  \n",
            "4          0          0          0          0          0          0  \n",
            "\n",
            "[5 rows x 785 columns]\n",
            "\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 60000 entries, 0 to 59999\n",
            "Columns: 785 entries, label to pixel_783\n",
            "dtypes: int64(785)\n",
            "memory usage: 359.3 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    \"\"\"\n",
        "    Compute the softmax probabilities for a given input matrix.\n",
        "\n",
        "    Parameters:\n",
        "    z (numpy.ndarray): Logits (raw scores) of shape (m, n), where\n",
        "                       - m is the number of samples.\n",
        "                       - n is the number of classes.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Softmax probability matrix of shape (m, n), where\n",
        "                   each row sums to 1 and represents the probability\n",
        "                   distribution over classes.\n",
        "\n",
        "    Notes:\n",
        "    - The input to softmax is typically computed as: z = XW + b.\n",
        "    - Uses numerical stabilization by subtracting the max value per row.\n",
        "    \"\"\"\n",
        "\n",
        "     # Subtract the maximum value per row for numerical stability\n",
        "    z_max = np.max(z, axis=1, keepdims=True)\n",
        "    z_stable = z - z_max\n",
        "\n",
        "    # Compute softmax\n",
        "    exp_z = np.exp(z_stable)\n",
        "    softmax_probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    return softmax_probs\n"
      ],
      "metadata": {
        "id": "YoOjTJJpt6Nv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Test Case:\n",
        "\n",
        "This test case checks that each row in the resulting softmax probabilities sums to 1, which is the fundamental property of softmax."
      ],
      "metadata": {
        "id": "ZFnMdHJzrUJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test case\n",
        "z_test = np.array([[2.0, 1.0, 0.1], [1.0, 1.0, 1.0]])\n",
        "softmax_output = softmax(z_test)\n",
        "\n",
        "# Verify if the sum of probabilities for each row is 1 using assert\n",
        "row_sums = np.sum(softmax_output, axis=1)\n",
        "\n",
        "# Assert that the sum of each row is 1\n",
        "assert np.allclose(row_sums, 1), f\"Test failed: Row sums are {row_sums}\"\n",
        "\n",
        "print(\"Softmax function passed the test case!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL5ToHmkrTr-",
        "outputId": "4401695f-f95c-4b34-dffc-cc48aa8e4f86"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax function passed the test case!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction Function:"
      ],
      "metadata": {
        "id": "j1uPYyhotoAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_softmax(X, W, b):\n",
        "    \"\"\"\n",
        "    Predict the class labels for a set of samples using the trained softmax model.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix of shape (n, d), where n is the number of samples and d is the number of features.\n",
        "    W (numpy.ndarray): Weight matrix of shape (d, c), where c is the number of classes.\n",
        "    b (numpy.ndarray): Bias vector of shape (c,).\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Predicted class labels of shape (n,), where each value is the index of the predicted class.\n",
        "    \"\"\"\n",
        "    # Compute logits (raw scores)\n",
        "    z = np.dot(X, W) + b\n",
        "\n",
        "    # Compute softmax probabilities\n",
        "    softmax_probs = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    softmax_probs /= np.sum(softmax_probs, axis=1, keepdims=True)\n",
        "\n",
        "    # Predict class labels (index of max probability per sample)\n",
        "    predicted_classes = np.argmax(softmax_probs, axis=1)\n",
        "\n",
        "    return predicted_classes\n"
      ],
      "metadata": {
        "id": "8qwCbgC1vyHn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Function for Prediction Function:\n",
        "The test function ensures that the predicted class labels have the same number of elements as the input samples, verifying that the model produces a valid output shape."
      ],
      "metadata": {
        "id": "LCGDTavVuXZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test case\n",
        "X_test = np.array([[0.2, 0.8], [0.5, 0.5], [0.9, 0.1]])  # Feature matrix (3 samples, 2 features)\n",
        "W_test = np.array([[0.4, 0.2, 0.1], [0.3, 0.7, 0.5]])  # Weights (2 features, 3 classes)\n",
        "b_test = np.array([0.1, 0.2, 0.3])  # Bias (3 classes)\n",
        "\n",
        "# Expected Output:\n",
        "# The function should return an array with class labels (0, 1, or 2)\n",
        "\n",
        "y_pred_test = predict_softmax(X_test, W_test, b_test)\n",
        "\n",
        "# Validate output shape\n",
        "assert y_pred_test.shape == (3,), f\"Test failed: Expected shape (3,), got {y_pred_test.shape}\"\n",
        "\n",
        "# Print the predicted labels\n",
        "print(\"Predicted class labels:\", y_pred_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "musr99YhucQX",
        "outputId": "14e3c7cd-9930-400a-92a7-b62b727f6fc5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class labels: [1 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function:"
      ],
      "metadata": {
        "id": "JwejxbajvEle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def loss_softmax(y_pred, y):\n",
        "    \"\"\"\n",
        "    Compute the cross-entropy loss for a single sample.\n",
        "\n",
        "    Parameters:\n",
        "    y_pred (numpy.ndarray): Predicted probabilities of shape (c,) for a single sample,\n",
        "                             where c is the number of classes.\n",
        "    y (numpy.ndarray): True labels (one-hot encoded) of shape (c,), where c is the number of classes.\n",
        "\n",
        "    Returns:\n",
        "    float: Cross-entropy loss for the given sample.\n",
        "    \"\"\"\n",
        "    # Avoid log(0) by adding a small value (epsilon) for numerical stability\n",
        "    epsilon = 1e-12\n",
        "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)  # Clip values to prevent log(0)\n",
        "\n",
        "    # Compute cross-entropy loss\n",
        "    loss = -np.sum(y * np.log(y_pred))\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "bjqnULCtun_Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test case for Loss Function:\n",
        "This test case Compares loss for correct vs. incorrect predictions.\n",
        "*   Expects low loss for correct predictions.\n",
        "*   Expects high loss for incorrect predictions."
      ],
      "metadata": {
        "id": "fXdMIV_cz5Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define correct predictions (low loss scenario)\n",
        "y_true_correct = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # True one-hot labels\n",
        "y_pred_correct = np.array([[0.9, 0.05, 0.05],\n",
        "                           [0.1, 0.85, 0.05],\n",
        "                           [0.05, 0.1, 0.85]])  # High confidence in the correct class\n",
        "\n",
        "# Define incorrect predictions (high loss scenario)\n",
        "y_pred_incorrect = np.array([[0.05, 0.05, 0.9],  # Highly confident in the wrong class\n",
        "                              [0.1, 0.05, 0.85],\n",
        "                              [0.85, 0.1, 0.05]])\n",
        "\n",
        "# Compute loss for both cases\n",
        "loss_correct = loss_softmax(y_pred_correct, y_true_correct)\n",
        "loss_incorrect = loss_softmax(y_pred_incorrect, y_true_correct)\n",
        "\n",
        "# Validate that incorrect predictions lead to a higher loss\n",
        "assert loss_correct < loss_incorrect, f\"Test failed: Expected loss_correct < loss_incorrect, but got {loss_correct:.4f} >= {loss_incorrect:.4f}\"\n",
        "\n",
        "# Print results\n",
        "print(f\"Cross-Entropy Loss (Correct Predictions): {loss_correct:.4f}\")\n",
        "print(f\"Cross-Entropy Loss (Incorrect Predictions): {loss_incorrect:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IhRGquu0N9P",
        "outputId": "73a0be59-001a-4095-aafb-20a08ee5a826"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Entropy Loss (Correct Predictions): 0.4304\n",
            "Cross-Entropy Loss (Incorrect Predictions): 8.9872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost Function:"
      ],
      "metadata": {
        "id": "y0d3fm1-vUlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_softmax(X, y, W, b):\n",
        "    \"\"\"\n",
        "    Compute the average softmax regression cost (cross-entropy loss) over all samples.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix of shape (n, d), where n is the number of samples and d is the number of features.\n",
        "    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c), where n is the number of samples and c is the number of classes.\n",
        "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "    b (numpy.ndarray): Bias vector of shape (c,).\n",
        "\n",
        "    Returns:\n",
        "    float: Average softmax cost (cross-entropy loss) over all samples.\n",
        "    \"\"\"\n",
        "    n = X.shape[0]  # Number of samples\n",
        "\n",
        "    # Compute logits (raw scores)\n",
        "    z = np.dot(X, W) + b\n",
        "\n",
        "    # Compute softmax probabilities with numerical stability\n",
        "    z_max = np.max(z, axis=1, keepdims=True)\n",
        "    exp_z = np.exp(z - z_max)\n",
        "    softmax_probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    # Avoid log(0) by adding a small epsilon for numerical stability\n",
        "    epsilon = 1e-12\n",
        "    softmax_probs = np.clip(softmax_probs, epsilon, 1.0 - epsilon)\n",
        "\n",
        "    # Compute cross-entropy loss\n",
        "    total_loss = -np.sum(y * np.log(softmax_probs))\n",
        "\n",
        "    # Return average loss\n",
        "    return total_loss / n\n"
      ],
      "metadata": {
        "id": "yaH9_s0svIGJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Case for Cost Function:\n",
        "The test case assures that the cost for the incorrect prediction should be higher than for the correct prediction, confirming that the cost function behaves as expected."
      ],
      "metadata": {
        "id": "-eGyPFJ33tgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example 1: Correct Prediction (Closer predictions)\n",
        "X_correct = np.array([[1.0, 0.0], [0.0, 1.0]])  # Feature matrix for correct predictions\n",
        "y_correct = np.array([[1, 0], [0, 1]])  # True labels (one-hot encoded, matching predictions)\n",
        "W_correct = np.array([[5.0, -2.0], [-3.0, 5.0]])  # Weights for correct prediction\n",
        "b_correct = np.array([0.1, 0.1])  # Bias for correct prediction\n",
        "\n",
        "# Example 2: Incorrect Prediction (Far off predictions)\n",
        "X_incorrect = np.array([[0.1, 0.9], [0.8, 0.2]])  # Feature matrix for incorrect predictions\n",
        "y_incorrect = np.array([[1, 0], [0, 1]])  # True labels (one-hot encoded, incorrect predictions)\n",
        "W_incorrect = np.array([[0.1, 2.0], [1.5, 0.3]])  # Weights for incorrect prediction\n",
        "b_incorrect = np.array([0.5, 0.6])  # Bias for incorrect prediction\n",
        "\n",
        "# Compute cost for correct predictions\n",
        "cost_correct = cost_softmax(X_correct, y_correct, W_correct, b_correct)\n",
        "\n",
        "# Compute cost for incorrect predictions\n",
        "cost_incorrect = cost_softmax(X_incorrect, y_incorrect, W_incorrect, b_incorrect)\n",
        "\n",
        "# Check if the cost for incorrect predictions is greater than for correct predictions\n",
        "assert cost_incorrect > cost_correct, f\"Test failed: Incorrect cost {cost_incorrect} is not greater than correct cost {cost_correct}\"\n",
        "\n",
        "# Print the costs for verification\n",
        "print(\"Cost for correct prediction:\", cost_correct)\n",
        "print(\"Cost for incorrect prediction:\", cost_incorrect)\n",
        "\n",
        "print(\"Test passed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIGAxYQt36Sr",
        "outputId": "4121ac9e-d23e-44bc-98f1-a2600045a1ef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost for correct prediction: 0.0006234364133349324\n",
            "Cost for incorrect prediction: 0.29930861359446115\n",
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing Gradients:"
      ],
      "metadata": {
        "id": "v-YIb7zlveKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_gradient_softmax(X, y, W, b):\n",
        "    \"\"\"\n",
        "    Compute the gradients of the cost function with respect to weights and biases.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix of shape (n, d).\n",
        "    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
        "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "    b (numpy.ndarray): Bias vector of shape (c,).\n",
        "\n",
        "    Returns:\n",
        "    tuple: Gradients with respect to weights (d, c) and biases (c,).\n",
        "    \"\"\"\n",
        "    n = X.shape[0]  # Number of samples\n",
        "\n",
        "    # Compute logits (raw scores)\n",
        "    z = np.dot(X, W) + b\n",
        "\n",
        "    # Compute softmax probabilities with numerical stability\n",
        "    z_max = np.max(z, axis=1, keepdims=True)\n",
        "    exp_z = np.exp(z - z_max)\n",
        "    softmax_probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute the gradient of loss with respect to softmax inputs (logits)\n",
        "    dz = softmax_probs - y  # Error term (difference between predicted and actual)\n",
        "\n",
        "    # Compute gradients\n",
        "    grad_W = np.dot(X.T, dz) / n  # Gradient with respect to weights\n",
        "    grad_b = np.sum(dz, axis=0) / n  # Gradient with respect to biases\n",
        "\n",
        "    return grad_W, grad_b\n"
      ],
      "metadata": {
        "id": "G3Vpn5bNvW3x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test case for compute_gradient function:\n",
        "The test checks if the gradients from the function are close enough to the manually computed gradients using np.allclose, which accounts for potential floating-point discrepancies."
      ],
      "metadata": {
        "id": "S84yoIUx7vY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a simple feature matrix and true labels\n",
        "X_test = np.array([[0.2, 0.8], [0.5, 0.5], [0.9, 0.1]])  # Feature matrix (3 samples, 2 features)\n",
        "y_test = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # True labels (one-hot encoded, 3 classes)\n",
        "\n",
        "# Define weight matrix and bias vector\n",
        "W_test = np.array([[0.4, 0.2, 0.1], [0.3, 0.7, 0.5]])  # Weights (2 features, 3 classes)\n",
        "b_test = np.array([0.1, 0.2, 0.3])  # Bias (3 classes)\n",
        "\n",
        "# Compute the gradients using the function\n",
        "grad_W, grad_b = compute_gradient_softmax(X_test, y_test, W_test, b_test)\n",
        "\n",
        "# Manually compute the predicted probabilities (using softmax function)\n",
        "z_test = np.dot(X_test, W_test) + b_test\n",
        "y_pred_test = softmax(z_test)\n",
        "\n",
        "# Compute the manually computed gradients\n",
        "grad_W_manual = np.dot(X_test.T, (y_pred_test - y_test)) / X_test.shape[0]\n",
        "grad_b_manual = np.sum(y_pred_test - y_test, axis=0) / X_test.shape[0]\n",
        "\n",
        "# Assert that the gradients computed by the function match the manually computed gradients\n",
        "assert np.allclose(grad_W, grad_W_manual), f\"Test failed: Gradients w.r.t. W are not equal.\\nExpected: {grad_W_manual}\\nGot: {grad_W}\"\n",
        "assert np.allclose(grad_b, grad_b_manual), f\"Test failed: Gradients w.r.t. b are not equal.\\nExpected: {grad_b_manual}\\nGot: {grad_b}\"\n",
        "\n",
        "# Print the gradients for verification\n",
        "print(\"Gradient w.r.t. W:\", grad_W)\n",
        "print(\"Gradient w.r.t. b:\", grad_b)\n",
        "\n",
        "print(\"Test passed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-YSC_Ot70bZ",
        "outputId": "4fdd9e06-e32b-4675-f5c2-3294e361cf92"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient w.r.t. W: [[ 0.1031051   0.01805685 -0.12116196]\n",
            " [-0.13600547  0.00679023  0.12921524]]\n",
            "Gradient w.r.t. b: [-0.03290036  0.02484708  0.00805328]\n",
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Gradient Descent:"
      ],
      "metadata": {
        "id": "W75VL71ivpjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent_softmax(X, y, W, b, alpha, n_iter, show_cost=False):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the weights and biases.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix of shape (n, d).\n",
        "    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
        "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "    b (numpy.ndarray): Bias vector of shape (c,).\n",
        "    alpha (float): Learning rate.\n",
        "    n_iter (int): Number of iterations.\n",
        "    show_cost (bool): Whether to display the cost at intervals.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Optimized weights, biases, and cost history.\n",
        "    \"\"\"\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        # Compute gradients\n",
        "        grad_W, grad_b = compute_gradient_softmax(X, y, W, b)\n",
        "\n",
        "        # Update weights and biases\n",
        "        W -= alpha * grad_W\n",
        "        b -= alpha * grad_b\n",
        "\n",
        "        # Compute cost for monitoring\n",
        "        cost = cost_softmax(X, y, W, b)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Display cost every 100 iterations (if enabled)\n",
        "        if show_cost and i % 100 == 0:\n",
        "            print(f\"Iteration {i}: Cost = {cost:.4f}\")\n",
        "\n",
        "    return W, b, cost_history\n"
      ],
      "metadata": {
        "id": "bbQ7SVw7vo-M"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Dataset:"
      ],
      "metadata": {
        "id": "zBG9uSWKHDgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_and_prepare_mnist(csv_file, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Reads the MNIST CSV file, splits data into train/test sets, and plots one image per class.\n",
        "\n",
        "    Arguments:\n",
        "    csv_file (str)       : Path to the CSV file containing MNIST data.\n",
        "    test_size (float)    : Proportion of the data to use as the test set (default: 0.2).\n",
        "    random_state (int)   : Random seed for reproducibility (default: 42).\n",
        "\n",
        "    Returns:\n",
        "    X_train, X_test, y_train, y_test : Split dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Separate labels and features\n",
        "    y = df.iloc[:, 0].values  # First column is the label\n",
        "    X = df.iloc[:, 1:].values  # Remaining columns are pixel values\n",
        "\n",
        "    # Normalize pixel values (optional but recommended)\n",
        "    X = X / 255.0  # Scale values between 0 and 1\n",
        "\n",
        "    # Split data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Plot one sample image per class\n",
        "    plot_sample_images(X, y)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def plot_sample_images(X, y):\n",
        "    \"\"\"\n",
        "    Plots one sample image for each digit class (0-9).\n",
        "\n",
        "    Arguments:\n",
        "    X (np.ndarray): Feature matrix containing pixel values.\n",
        "    y (np.ndarray): Labels corresponding to images.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    unique_classes = np.unique(y)  # Get unique class labels\n",
        "\n",
        "    for i, digit in enumerate(unique_classes):\n",
        "        index = np.where(y == digit)[0][0]  # Find first occurrence of the class\n",
        "        image = X[index].reshape(28, 28)  # Reshape 1D array to 28x28\n",
        "\n",
        "        plt.subplot(2, 5, i + 1)\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.title(f\"Digit: {digit}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "prZ_zAvLpodE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = \"/content/mnist_dataset_2025.csv\"  # Path to saved dataset\n",
        "X_train, X_test, y_train, y_test = load_and_prepare_mnist(csv_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "ZtYR42Qas2uf",
        "outputId": "5e813934-8272-476e-eb15-f8c87a91572f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A Quick debugging Step:**"
      ],
      "metadata": {
        "id": "MyMBH4mQtzHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assert that X and y have matching lengths\n",
        "assert X_train.shape[0] == y_train.shape[0], f\"Error: X and y have different lengths! X={X_train.shape[0]}, y={y_train.shape[0]}\"\n",
        "\n",
        "print(\"Move forward: Dimension of Feature Matrix X and label vector y matched.\")\n",
        "\n",
        "# Assert that X and y have matching lengths\n",
        "assert X_train.shape[0] == y_train.shape[0], f\"Error: X and y have different lengths! X={X_train.shape[0]}, y={y_train.shape[0]}\"\n",
        "\n",
        "print(\"Move forward: Dimension of Feature Matrix X and label vector y matched.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "QIJhtnuCs7QF",
        "outputId": "9078fb0f-daa5-4a30-9466-6e5c2a142c45"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-08655cf3be7e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cell 2: Asserting Dimension Match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Assert that X and y have matching lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Error: X and y have different lengths! X={X_train.shape[0]}, y={y_train.shape[0]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Move forward: Dimension of Feature Matrix X and label vector y matched.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assert that X and y have matching lengths\n",
        "assert len(X_train) == len(y_train), f\"Error: X and y have different lengths! X={len(X_train)}, y={len(y_train)}\"\n",
        "print(\"Move forward: Dimension of Feture Matrix X and label vector y matched.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "208e7878-4f9d-4410-fc87-de670f8564cd",
        "id": "ITEPlSAZEgib"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c0bddc994d13>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assert that X and y have matching lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Error: X and y have different lengths! X={len(X_train)}, y={len(y_train)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Move forward: Dimension of Feture Matrix X and label vector y matched.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train the Model:**"
      ],
      "metadata": {
        "id": "-TKIsKJcwFsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "fEuTbCU0xAQW",
        "outputId": "aae8aecf-f794-48de-9e80-0d9c82389602"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-b28f2a5edf33>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training data shape: {X_train.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test data shape: {X_test.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Check if y_train is one-hot encoded\n",
        "if len(y_train.shape) == 1:\n",
        "    encoder = OneHotEncoder(sparse_output=False)  # Use sparse_output=False for newer versions of sklearn\n",
        "    y_train = encoder.fit_transform(y_train.reshape(-1, 1))  # One-hot encode labels\n",
        "    y_test = encoder.transform(y_test.reshape(-1, 1))  # One-hot encode test labels\n",
        "\n",
        "# Now y_train is one-hot encoded, and we can proceed to use it\n",
        "d = X_train.shape[1]  # Number of features (columns in X_train)\n",
        "c = y_train.shape[1]  # Number of classes (columns in y_train after one-hot encoding)\n",
        "\n",
        "# Initialize weights with small random values and biases with zeros\n",
        "W = np.random.randn(d, c) * 0.01  # Small random weights initialized\n",
        "b = np.zeros(c)  # Bias initialized to 0\n",
        "\n",
        "# Set hyperparameters for gradient descent\n",
        "alpha = 0.1  # Learning rate\n",
        "n_iter = 1000  # Number of iterations to run gradient descent\n",
        "\n",
        "# Train the model using gradient descent\n",
        "W_opt, b_opt, cost_history = gradient_descent_softmax(X_train, y_train, W, b, alpha, n_iter, show_cost=True)\n",
        "\n",
        "# Plot the cost history to visualize the convergence\n",
        "plt.plot(cost_history)\n",
        "plt.title('Cost Function vs. Iterations')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "J8e2mHmRv4fd",
        "outputId": "35916520-b730-44bd-9bb3-571d8adcfecb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e03968e9939e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Check if y_train is one-hot encoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use sparse_output=False for newer versions of sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# One-hot encode labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating the Model:**"
      ],
      "metadata": {
        "id": "tH4wNbhzys4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_classification(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluate classification performance using confusion matrix, precision, recall, and F1-score.\n",
        "\n",
        "    Parameters:\n",
        "    y_true (numpy.ndarray): True labels\n",
        "    y_pred (numpy.ndarray): Predicted labels\n",
        "\n",
        "    Returns:\n",
        "    tuple: Confusion matrix, precision, recall, F1 score\n",
        "    \"\"\"\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Compute precision, recall, and F1-score\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    return cm, precision, recall, f1\n"
      ],
      "metadata": {
        "id": "lzV7BkRqOl5A"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred_test = predict_softmax(X_test, W_opt, b_opt)\n",
        "\n",
        "# Evaluate accuracy\n",
        "y_test_labels = np.argmax(y_test, axis=1)  # True labels in numeric form\n",
        "\n",
        "# Evaluate the model\n",
        "cm, precision, recall, f1 = evaluate_classification(y_test_labels, y_pred_test)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Visualizing the Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "cax = ax.imshow(cm, cmap='Blues')  # Use a color map for better visualization\n",
        "\n",
        "# Dynamic number of classes\n",
        "num_classes = cm.shape[0]\n",
        "ax.set_xticks(range(num_classes))\n",
        "ax.set_yticks(range(num_classes))\n",
        "ax.set_xticklabels([f'Predicted {i}' for i in range(num_classes)])\n",
        "ax.set_yticklabels([f'Actual {i}' for i in range(num_classes)])\n",
        "\n",
        "# Add labels to each cell in the confusion matrix\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, cm[i, j], ha='center', va='center', color='white' if cm[i, j] > np.max(cm) / 2 else 'black')\n",
        "\n",
        "# Add grid lines and axis labels\n",
        "ax.grid(False)\n",
        "plt.title('Confusion Matrix', fontsize=14)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('Actual Label', fontsize=12)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.colorbar(cax)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "uuGtvIlywK7J",
        "outputId": "505a03f0-24a1-47e4-f0be-679d3be4fdbc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'W_opt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-b2673c128f28>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predict on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Evaluate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_test_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# True labels in numeric form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'W_opt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Seperability and Logistic Regression:"
      ],
      "metadata": {
        "id": "yv8J5hNCPzl6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_-uDdFRGgAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}